# 3月份文献阅读(Prompt tuning and Verbalizers)

整理人：陈皓_10205102429



## 1. Knowledgable Prompt Tuning

**论文链接**：[Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://aclanthology.org/2022.acl-long.158.pdf)

**发表会议**：2022 ACL

**发表单位（院校）**：清华大学

**一作**：Shengding Hu

### 1.1 主体思想

本文主要针对prompt-tuning中的verbalizer部分进行修改，通过引入额外的知识扩展标签词空间，为了更好的利用知识利用PLM重新定义了扩展后的标签词空间。

### 1.2 模型架构

![1](image/1.png)

**备注说明**：

1. 标签词空间通过已经有的知识图谱被拓展，一个标签对应的词语增多了，拓宽了知识的广度。
2. 预训练语言模型在标签词上预测的概率会被最终转化成标签的概率，哪一个更高，则分类为哪一个标签。
3. 针对于广泛的知识源，词语经过了一定量的精细化（Refine），得出最终所需要运用的知识。
4. 由于人工设计的模板已被证明比自动学习出的模板更加有效，每个数据集使用了4个人工设计的模板，报告4个模板的评价结果和最佳结果
5. KPT始终优于PT，特别是在5-shot和10-shot实验中
6. 对于20-shot，认为标签实例的数量足以优化标签词的嵌入，使其远离原来的词嵌入，从而使知识丰富的语义，所以引入知识的verbalizer提供较少的帮助

### 1.3 待改进之处

1. 用更精细化的方法从verbalizer中选择有用的标签词。（预先进行过实验，发现在agnews数据集上较少的知识嵌入也能够取得很好的效果）
2. 在模板构造和verbalizer设计方面用更好方法结合知识库和prompt-tuning
3. 如何构造好一个好的知识嵌入的verbalizer，以及对于verbalizer的功能和架构进行设计，可作为一个研究的方向。



## 2. Pattern-Exploiting Training(PET)

**论文链接**：[Exploiting Cloze Questions for Few Shot Text Classifification and Natural Language Inference](https://aclanthology.org/2021.eacl-main.20.pdf)

**发表会议**：2021 EACL

**发表单位（院校）**：慕尼黑大学信息和语言处理中心

**一作**：Timo Schick

### 1.1 主体思想

提出了PET的训练模式以及verbalizer的概念，对于prompt tuning用在分类问题中有着比较显著的意义。

当在小样本学习的领域，有task description存在的情况下，任务的解决会变得更加简单。

### 1.2 模型架构

![2](image/2.png)

该图表示的是PET工作的三个步骤：

1. 首先，对于每个模式，一个单独的 PLM 在一个小训练集 T 上进行微调。
2. 然后使用所有模型的集合来注释带有软标签的大型未标记数据集 D。
3. 最后，在软标记数据集上训练标准分类器。我们还设计了 iPET，这是 PET 的一种迭代变体，其中随着训练集大小的增加重复此过程。
