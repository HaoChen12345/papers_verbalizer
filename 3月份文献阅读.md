# 3月份文献阅读(Prompt tuning and Verbalizers)

整理人：陈皓_10205102429



## 1. Knowledgable Prompt Tuning

**论文链接**：https://aclanthology.org/2022.acl-long.158.pdf

**发表会议**：2022 ACL

**发表单位（院校）**：清华大学

### 1.1 主体思想

本文主要针对prompt-tuning中的verbalizer部分进行修改，通过引入额外的知识扩展标签词空间，为了更好的利用知识利用PLM重新定义了扩展后的标签词空间。

### 1.2 模型架构

![1](image\1.png)

**备注说明**：

1. 标签词空间通过已经有的知识图谱被拓展，一个标签对应的词语增多了，拓宽了知识的广度。
2. 预训练语言模型在标签词上预测的概率会被最终转化成标签的概率，哪一个更高，则分类为哪一个标签。
3. 针对于广泛的知识源，词语经过了一定量的精细化（Refine），得出最终所需要运用的知识。
4. 由于人工设计的模板已被证明比自动学习出的模板更加有效，每个数据集使用了4个人工设计的模板，报告4个模板的评价结果和最佳结果
5. KPT始终优于PT，特别是在5-shot和10-shot实验中
6. 对于20-shot，认为标签实例的数量足以优化标签词的嵌入，使其远离原来的词嵌入，从而使知识丰富的语义，所以引入知识的verbalizer提供较少的帮助

### 1.3 待改进之处

1. 用更精细化的方法从verbalizer中选择有用的标签词。（预先进行过实验，发现在agnews数据集上较少的知识嵌入也能够取得很好的效果）
2. 在模板构造和verbalizer设计方面用更好方法结合知识库和prompt-tuning
3. 如何构造好一个好的知识嵌入的verbalizer，以及对于verbalizer的功能和架构进行设计，可作为一个研究的方向。