# 3月份文献阅读(Prompt tuning and Verbalizers)

整理人：陈皓_10205102429

**小结**：近期的工作方向是prompt tuning在知识嵌入方面的微调，阅读文献的重点为prompt tuning整体结构的设计和verbalizer的相关设计方法。（关键于运用知识）

## 1. Knowledgable Prompt Tuning

**论文链接**：[Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://aclanthology.org/2022.acl-long.158.pdf)

**发表会议**：2022 ACL

**发表单位（院校）**：清华大学

**一作**：Shengding Hu

### 1.1 主体思想

本文主要针对prompt-tuning中的verbalizer部分进行修改，通过引入额外的知识扩展标签词空间，为了更好的利用知识利用PLM重新定义了扩展后的标签词空间。

### 1.2 模型架构

![1](image/1.png)

(science和sports均有对应的外部知识，从中判断属于该类别的概率)

**备注说明**：

1. 标签词空间通过已经有的知识图谱被拓展，一个标签对应的词语增多了，拓宽了知识的广度。
2. 预训练语言模型在标签词上预测的概率会被最终转化成标签的概率，哪一个更高，则分类为哪一个标签。
3. 针对于广泛的知识源，词语经过了一定量的精细化（Refine），得出最终所需要运用的知识。
4. 由于人工设计的模板已被证明比自动学习出的模板更加有效，每个数据集使用了4个人工设计的模板，报告4个模板的评价结果和最佳结果
5. KPT始终优于PT，特别是在5-shot和10-shot实验中
6. 对于20-shot，认为标签实例的数量足以优化标签词的嵌入，使其远离原来的词嵌入，从而使知识丰富的语义，所以引入知识的verbalizer提供较少的帮助

**实验结果：**

**zero-shot**

![11](image/11.png)

**few-shot**

![12](image/12.png)

### 1.3 读后感

1. 用更精细化的方法从verbalizer中选择有用的标签词。（预先进行过实验，发现在agnews数据集上较少的知识嵌入也能够取得很好的效果）
2. 在模板构造和verbalizer设计方面用更好方法结合知识库和prompt-tuning
3. 如何构造好一个好的知识嵌入的verbalizer，以及对于verbalizer的功能和架构进行设计，可作为一个研究的方向。
4. <u>对比实验数据，KPT在少样本学习的领域甚至比不上PET，而PET并没有运用太多的外部知识。因此笔者认为，这篇论文在考虑外部知识运用的有效性上有所欠缺。从直观感受上，术业有专攻，不同的人面对不同的任务表现是会有差异。如何有效利用知识可能可以作为一个提升的方向。</u>



## 2. Pattern-Exploiting Training(PET)

**论文链接**：[Exploiting Cloze Questions for Few Shot Text Classifification and Natural Language Inference](https://aclanthology.org/2021.eacl-main.20.pdf)

**发表会议**：2021 EACL（实际上是2020年写的）

**发表单位（院校）**：慕尼黑大学信息和语言处理中心

**一作**：Timo Schick

### 2.1 主体思想

提出了PET的训练模式以及verbalizer的概念，对于prompt tuning用在分类问题中有着比较显著的意义。

当在小样本学习的领域，有task description存在的情况下，任务的解决会变得更加简单。

### 2.2 模型架构

![2](image/2.png)

该图表示的是PET工作的三个步骤：

1. 首先，对于每个模式，一个单独的 PLM 在一个小训练集 T 上进行微调。
2. 然后使用所有模型的集合来注释带有软标签的大型未标记数据集 D。（打上的是一个soft label）
3. 最后，在软标记数据集上训练标准分类器。作者还设计了 iPET，这是 PET 的一种迭代变体，其中随着训练集大小的增加重复此过程。

**PVP**：一个(P,v)组成的元组对

P(x)是一个模式，将输入的句子包装。

![3](image/3.png)

![4](image/4.png)

v是一个Verbalizer，将分类问题中的类别映射到要填到空格的词当中（感觉通常上像是单射）。

如正例映射到“Yes”，负例映射到“No”

训练定义：定义一个 M(w∣Z) 表示给定带有一个[MASK]标记的序列 Z，语言模型可以在该[MASK]位置填入词 w∈L 的非归一化得分，即：(M是一个评分函数)

![5](image/5.png)

其次定义概率分布(类似于softmax的分布，最终转化成概率的问题）。

![6](image/6.png)

总的损失函数：

![7](image/7.png)

**对于给定的PVP，一个难题是如何确定哪一个PVP表现的好，因此需要走以下三步**：（类似于知识蒸馏的方法）

1. 每一个PVP都在一个小的数据集上进行微调。
2. 将每一个预训练模型进行集成、对于每个预训练模型，分别对未标注的数据集 D 进行标注，此时获得的是soft-label，即给定一个输入 X，标签 l∈V 的概率得分：（这个w是权重，Z是权重和，得分走一层softmax被转化成概率分布，用T=2去保持一个soft distribution。打好软标签的数据集为Tc）![8](image/8.png)
3. 在 Tc 使用标准的微调方法进行微调。微调之后的模型即作为最终的分类器

(有点像知识蒸馏)

**迭代方式的PEI（iPET）**：

![9](image/9.png)

**实验结果**：

![10](image/10.png)

### 2.3 读后感

这篇文献是2021年提出的，但感觉其中的一些方法在相关的数据规模上已经超过了2022年提出来的KPT



## 3. Automatic Verbalizer in Text Classification

**论文链接**：[Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification](https://aclanthology.org/2020.coling-main.488.pdf)

**发表会议**：2020 COLING

**发表单位（院校）**：慕尼黑大学信息和语言处理中心

**一作**：Timo Schick

### 3.1 主体思想

依据文章题目，本文的主要贡献是提供了一种自动搜索的方式来决定哪些词可以作为分类问题中的label。

本文可以看成作者对自己第2篇文章的一个拓展。

在上文中，手动定义单词和标签之间的映射需要领域的专业知识和对语言模型能力的理解。本文探寻自动寻找的方式和技术。

本文提出的方法为PETAL（PET *with Automatic Labels* ）。表现可以达到和手工定义的映射一样好的效果。

### 3.2 模型架构

PET模型示例：

![13](image/13.png)

文章花了不小的一部分篇幅来重新叙述PET的架构和训练方式。

极大似然估计来寻找最好的verbalizer：

![14](image/14.png)

将k分类问题转移成k个二分类的问题，因此每个分类任务的训练集调整为：

![15](image/15.png)

定义概率分布：

![16](image/16.png)

此时满足$q_{(P,t)}(0|x)=1-q_{(P,t)}(1|x)$

考虑到k分类问题被分为k个二分类问题，每个二分类问题中的标签都有不均衡的情况。为保证均衡，需赋权重：(相似的方法在2001年已经提出来了)

![18](image/18.png)

采用LR(likelihood Ratio)作为损失函数：

![19](image/19.png)

关于作为标签的候选词，作者创建了只包含词汇表的一小部分子集的候选集Vy⊂T：长度大于1的词不要，随后再找10000个最常见的token作为候选集合，同时需要更加偏好于frequent words

生成标签词示例：

![21](image/21.png)

**实验结果**：

![20](image/20.png)

### 3.3 读后感

本文介绍了一种自动搜索label单词的方式，可以有效避免人工设计verbalizer的麻烦。可能可以用于决定哪些外部知识的词语更加合适的情况。

这里词语的来源是实验预设的词表，并不是来自于知识图谱中的外部知识。



## 4.Prompt Tuning with Rules(PTR)

**论文链接**：[PTR: Prompt Tuning with Rules for Text Classification](https://reader.elsevier.com/reader/sd/pii/S2666651022000183?token=F9CF2B34BBD6B9FA503D59FF7C18D82A662B22C2287E8E7167A24F45A9B101E518B64A8BB5D606EA64596BB03BFAF672&originRegion=us-east-1&originCreation=20230321061558)

**发表会议**：AI Open(2022)

**发表单位（院校）**：清华大学

**一作**：Xu Han 

### 4.1 主体思想

缩小预训练和下游任务fine-tune的差距。

### 4.2 模型架构

pre-train，fine-tune和prompt tuning的差别

![22](image/22.png)

PTR的架构：（感觉用在关系的判断上会比较好）

![23](image/23.png)

### 4.3 读后感

PTR用在关系抽取和关系分类的问题上会有比较好的效果，但感觉对于sub-task的划分和界定，作者没有叙述的太清楚。此处作为一个参考。

## 5. Late Prompt Tuning

**论文链接**：[Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts](https://aclanthology.org/2022.findings-emnlp.95.pdf)

**发表会议**：2022 EMNLP

**发表单位（院校）**：复旦大学

**一作**：Xiangyang Liu

### 5.1 主体思想

本文通过大量有力的实验论证了**long propagation path from label signals to the input prompt**导致了prompt tuning的表现非常不好，对此，作者通过改变prompt tuning及transformer架构相关的部分，从而提升prompt tuning的性能，同时也减少了微调的参数和训练的开销。

相关实验的记录如下：

![25](image/25.png)

### 5.2 模型架构

![24](image/24.png)

### 5.3 读后感

对于late prompt tuning 的架构和相关代码尚未理解透彻，但知道其好处巨大。倘若在verbalizer改进的过程中使用该技术，可能获得一个不错的效果。



## 6. S-BERT-KG

全称：(Sentence-based BERT with Knowledge Graph Embedding)

**论文链接**：[Zero-Shot Text Classifification via Knowledge Graph Embedding for Social Media Data](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9466939)

**发表会议**：2022 IEEE

### 6.1 主体思想

将知识图谱作为embedding，嵌入到预训练语言模型当中进行训练。该文训练的数据集来源主要是社交媒体、

### 6.2 模型架构

![26](image/26.png)

## 7. Cross-domain knowledge distillation

**论文链接**：[Cross-domain knowledge distillation for text classifification](https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231222X00350/1-s2.0-S092523122201058X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDrkSrrMm1BpRGwEuje1pxh1Bi37aLn44%2FLLeHGRFTpfAIhAOFzq5gnJElxKH95D5a3JNLjnYjzMkCVcCTTGLBCN459KrwFCIv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igz07r7leOlsVW7GwnkqkAVIlgJ800cDvRKibCKq%2B3y2YOI6KKuCp98emLz%2F0yQypaOQ%2BPo0MyXX77QG0jf%2F0LKX%2FJ6%2FTm51kL34SfXjlQdavRNL98%2BzxaSvCT4%2FKrXgdK6rfe%2FIZ1%2BBcOWpC%2FdkN%2FKnAuBW5nzGVquPczn8WjNPh3A%2F4GZh4%2FSA5uZI%2Fzs4DUan7%2FAxXF5MSLEhjHonE%2FrrIz8jHSPhk0irm68P4lX9M6indo63XD%2Bqbld3Itgsjsdus%2F0%2FYIyiXGC2iSTNpDF9byMcUNw%2BqePrwuYtF9pAIOuhWrir1tksY1JCjWMoJUASpkYuLQJSkP3c7wv1D5ZVSP1Xfm%2BZ7nJait4E08IcO7VMtodPA2Ue%2BJ7%2B3D%2B%2FjqdzZtbolJf%2BbY09dKK2GUzqYNxv8yq1%2F7%2FuNz2wiHFX1%2F6XiL72BdAQFQ8TuJOmFPqYPHex5pTnuoc4PjwayprM%2Bq5HWL3eUy4rK9DWvaX8AHqdLc7qVgFqDYYNoKCB676TyI7CQlraJXHIdHPPmVOr2KUc76T1IhWJc%2FpVGGnZRmV4ZAfVniaO00wCaKxS3PX6mgG9vCpbXXRgmBPz5%2FjnN8ViRuVF93WWEDfutznn9f1g%2BPa38ZoO0n2sfZvg%2FSotAX4w32N%2FaOtnikKbylMdjYYiti5xs8Ge0BsgBHi5YrwznA0W6jxD0FzjCvnYTP2CrX5INoFdvxmGyN7gsiZQUUK6k06XLMosyAWhOF4SCDJNIDys0wrm1uZTxa7xBMEX5z8gdh%2FbKKl%2FQY33ppU%2BKzCLhEkgyXkdhYebN1gsiloY4%2FaYAqrNvtwS41ivQwbTbkGdgyFnweuOaOiZrcftdBWT%2Fb1XKzgMZZkkUNif471RUn6Xfd98UeaPQpIzRzCfhOagBjqwATTkOrYJ8knZvptUBSsLqLy%2BTTZKLStgUHVRqXmwcpFb12%2BwpsfyQpZMpgpyGs4H89C66fQgQiNrIjQCxirUdcu8du3YQZcAH0a4CqVGV8OloJGf62dOJX3ChbYMSjaCvbLcfHkIg2utOiiKgxkbJVHfuF3Npa4i7GW9u6tNEfcHk9fV4G0Sl7sytEkM0JAqxD1lCzquec%2BMZ%2FPY%2FKhfSYyCFD0Vbg%2FMqIU570YifWV%2B&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230321T111531Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4U5OTVM4%2F20230321%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b10342ae0a35c0db78e8e7093b6d39c35fed3c0f1f6dd696280da1e5734a6b08&hash=e7523387a5d3a70033d41d95836c6366ab04a7821ff3dd4f5d1b689c24d53d62&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S092523122201058X&tid=spdf-e46803e9-7b36-473e-9002-721070983e50&sid=53e8d46d424b77458d3be3727fcb6d06bf28gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=050d5902065207555502&rr=7ab5c70c08d6fa26&cc=cn)

**发表会议**：Neurocomputing 2022

**发表单位**：中科院

### 7.1 主体思想

将跨领域的知识蒸馏的方法运用在文本分类当中。

### 7.2 模型架构

![27](image/27.png)

![28](image/28.png)

![29](image/29.png)

### 7.3 读后感

知识蒸馏是一种压缩大模型的方式，需要用到Teacher模型和Student模型。该领域的技术和方法或许能提供启发，但是还没有细看。
